{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Bio import Phylo, SeqIO\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from io import StringIO\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import weighting_methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in and processing test-trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "# tree = Phylo.read('../../Tree_rooting/Data/raw_OMA_trees/OMAGroup_479938.mafft.afa.treefile.Rooted.MPAJH', 'newick')\n",
    "tree = Phylo.read('/Users/adamhockenberry/Downloads/BM_Folder/paper_tree.txt', 'newick')\n",
    "# tree = Phylo.read(StringIO('(((A:20, B:20):30,C:50):30, D:80)'), 'newick', rooted=False)\n",
    "# tree = Phylo.read('../../Tree_rooting/Data/euk_trees/KOG0001.faa.aln.nwk.Rooted.MADAJH', 'newick')\n",
    "# tree = Phylo.read('../../Phylogenetic_couplings/Data/psicov150_aln_pdb/raw_trees/1a3aA.newick', 'newick')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(len(tree.get_terminals()))\n",
    "# tree = trim_zero_bls(tree)\n",
    "# tree.root_at_midpoint()\n",
    "# print(len(tree.get_terminals()))\n",
    "# initial_order = tree.get_terminals()\n",
    "# dicty = {}\n",
    "# for i,j in enumerate(initial_order):\n",
    "#     dicty[j.name] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.is_bifurcating()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree.get_terminals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(tree.get_terminals()) < 100:\n",
    "    Phylo.draw(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using root, get basic variance-covariance method ala Felsenstein\n",
    "\n",
    "So this is the basic method proposed in Altschull et al (ACL) and it works in my implementation (assuming no zero branch lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree.root_at_midpoint()\n",
    "weights_dict_acl, rooted_tree = weighting_methods.ACL_adhock(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sum(list(weights_dict_acl.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stone and Sidow proposed \"Branch_Manager\" which I at one point implemented and subsequently deleted. Their code works and at the moment I don't see the point.\n",
    "\n",
    "I, of course, deleted solely because I've gotten a much better grasp on these methods so if I decide to re-write don't want to even deal with the previous monstrosity that surely existed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GSC implementation!\n",
    "\n",
    "There are a few free parameter choices that I haven't fully settled on so as of now GSC methods are really a cluster of related methods all producing slightly different outputs.\n",
    "\n",
    "Of note, GSC values are highly dependent on the location of the tree root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights_dict = weighting_methods.GSC_adhock(tree)\n",
    "normed_weights_dict = weighting_methods.normalize_GSC_weights(weights_dict, tree)\n",
    "print(np.sum([i[-1] for i in weights_dict.values()]), tree.total_branch_length())\n",
    "print(np.sum([i[-1] for i in normed_weights_dict.values()]), tree.total_branch_length())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights_dict_v2 = weighting_methods.GSC_adhock_modified(tree)\n",
    "normed_weights_dict_v2 = weighting_methods.normalize_GSC_weights(weights_dict_v2, tree)\n",
    "\n",
    "print(np.sum([i[-1] for i in weights_dict_v2.values()]), tree.total_branch_length())\n",
    "print(np.sum([i[-1] for i in normed_weights_dict_v2.values()]), tree.total_branch_length())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = [i[-1] for i in weights_dict.values()]\n",
    "b = [i[-1] for i in normed_weights_dict.values()]\n",
    "c = [i[-1] for i in weights_dict_v2.values()]\n",
    "d = [i[-1] for i in normed_weights_dict_v2.values()]\n",
    "e = [i for i in weights_dict_acl.values()]\n",
    "\n",
    "a = np.array(a)/np.mean(a)\n",
    "b = np.array(b)/np.mean(b)\n",
    "c = np.array(c)/np.mean(c)\n",
    "d = np.array(d)/np.mean(d)\n",
    "e = np.array(e)/np.mean(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax_arr = plt.subplots(ncols=2, nrows=1)\n",
    "ax_arr[0].hist(a)\n",
    "ax_arr[0].hist(b)\n",
    "ax_arr[1].hist(c)\n",
    "ax_arr[1].hist(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.std(a), np.std(b), np.std(c), np.std(d), np.std(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for term_a in tree.get_terminals():\n",
    "    for term_b in tree.get_terminals():\n",
    "        print(term_a.name, term_b.name, (weights_dict_v2[term_a][-1]*weights_dict_v2[term_b][-1])/\\\n",
    "        (2*np.sqrt(weights_dict_v2[term_a][-1]*weights_dict_v2[term_b][-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Henikoff weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "from collections import Counter\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# records = list(SeqIO.parse('../../Tree_rooting/Data/Tria_et_al_data/'\n",
    "#                            'eukaryotes/ingroup/aln/KOG0018.faa.aln', 'fasta'))\n",
    "# tree = Phylo.read('../../Tree_rooting/Data/Tria_et_al_data/'\n",
    "#                   'eukaryotes/processed_trees/KOG0018.faa.aln.nwk.Rooted.MADAJH', 'newick')\n",
    "\n",
    "records = list(SeqIO.parse('../../Tree_rooting/Data/OMA_group_data/eukaryotes/aligned_OMA_groups/'\n",
    "                           'OMAGroup_833097.mafft.afa', 'fasta'))\n",
    "tree = Phylo.read('../../Tree_rooting/Data/OMA_group_data/eukaryotes/processed_OMA_trees/'\n",
    "                           'OMAGroup_833097.treefile.Rooted.MADAJH', 'newick')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(records))\n",
    "seqs = np.array([list(record.seq) for record in records])\n",
    "print(seqs.shape)\n",
    "seqs_T = seqs.T\n",
    "print(seqs_T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# seqs = np.array([list('GYVGS'),\n",
    "#                  list('GFDGF'),\n",
    "#                  list('GYDGF'),\n",
    "#                  list('GYQGG')])\n",
    "# seqs_T = seqs.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights_T = []\n",
    "all_weights = []\n",
    "for i in seqs_T[:]:\n",
    "    counter_dict = Counter(i)\n",
    "    del counter_dict['-']\n",
    "    r = len(counter_dict.keys())\n",
    "    positions = np.sum(list(counter_dict.values()))\n",
    "    weights_dict = {}\n",
    "    for key, val in counter_dict.items():\n",
    "    ####Adjust (or don't) according to the percentage of gaps in the sequence\n",
    "        weights_dict[key] = 1./(r*val)\n",
    "    temp_array = np.zeros(i.shape)\n",
    "    for key, val in weights_dict.items():\n",
    "        np.place(temp_array, i==key, [val])\n",
    "    temp_array = temp_array * (positions/seqs_T.shape[1])\n",
    "    weights_T.append(temp_array)\n",
    "weights_T = np.array(weights_T)\n",
    "all_weights = weights_T.T\n",
    "all_weights = np.sum(all_weights, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_weights = all_weights/np.mean(all_weights)\n",
    "print(np.sum(all_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# all_weights = np.sum(all_weights, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# all_weights = all_weights_T.T\n",
    "# all_weights = np.sum(all_weights, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights_dict = weighting_methods.GSC_adhock(tree)\n",
    "normed_weights_dict = weighting_methods.normalize_GSC_weights(weights_dict, tree)\n",
    "# acl_dict, x = weighting_methods.ACL_adhock(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = []\n",
    "b = []\n",
    "c = []\n",
    "# d = []\n",
    "for i, record in enumerate(records):\n",
    "#     if tree.find_any(record.id) not in acl_dict.keys():\n",
    "#         continue\n",
    "    a.append(all_weights[i])\n",
    "    b.append(weights_dict[tree.find_any(record.id)][-1])\n",
    "    c.append(normed_weights_dict[tree.find_any(record.id)][-1])\n",
    "#     d.append(acl_dict[tree.find_any(record.id)])\n",
    "\n",
    "\n",
    "a = np.array(a)/np.mean(a)\n",
    "b = np.array(b)/np.mean(b)\n",
    "c = np.array(c)/np.mean(c)\n",
    "# d = np.array(d)/np.mean(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax_arr = plt.subplots(ncols=3, figsize=(16,3))\n",
    "ax_arr[0].plot(a,b, 'bo')\n",
    "ax_arr[1].plot(a,c, 'bo')\n",
    "ax_arr[2].plot(b,c, 'bo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(stats.spearmanr(a,b),'\\n',stats.spearmanr(a,c),'\\n',stats.spearmanr(b,c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(stats.spearmanr(a,d),'\\n',stats.spearmanr(b,d),'\\n',stats.spearmanr(c,d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax_arr = plt.subplots(nrows=2, ncols=2, figsize=(12,8))\n",
    "ax_arr[0,0].hist(a,normed=True)\n",
    "ax_arr[0,1].hist(b, normed=True)\n",
    "ax_arr[1,0].hist(c, normed=True)\n",
    "ax_arr[1,1].hist(d, normed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Krogh and Mitchison max-ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# records = list(SeqIO.parse('../../Tree_rooting/Data/Tria_et_al_data/'\n",
    "#                            'eukaryotes/ingroup/aln/KOG0018.faa.aln', 'fasta'))\n",
    "# tree = Phylo.read('../../Tree_rooting/Data/Tria_et_al_data/'\n",
    "#                   'eukaryotes/processed_trees/KOG0018.faa.aln.nwk.Rooted.MADAJH', 'newick')\n",
    "\n",
    "# records = list(SeqIO.parse('../../Tree_rooting/Data/OMA_group_data/eukaryotes/aligned_OMA_groups/'\n",
    "#                            'OMAGroup_833097.mafft.afa', 'fasta'))\n",
    "# tree = Phylo.read('../../Tree_rooting/Data/OMA_group_data/eukaryotes/processed_OMA_trees/'\n",
    "#                            'OMAGroup_833097.treefile.Rooted.MADAJH', 'newick')\n",
    "\n",
    "records = list(SeqIO.parse('../../Phylogenetic_couplings/Data/psicov150_aln_pdb/'\n",
    "                           'aln_fasta_max1k/1aoeA.fasta', 'fasta'))\n",
    "tree = Phylo.read('../../Phylogenetic_couplings/Data/psicov150_aln_pdb/'\n",
    "                  'mp_root_trees/1aoeA.newick', 'newick')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(records))\n",
    "seqs = np.array([list(record.seq) for record in records])\n",
    "print(seqs.shape)\n",
    "seqs_T = seqs.T\n",
    "print(seqs_T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "initial_shape = seqs_T.shape\n",
    "flat_seqs = seqs_T.flatten()\n",
    "order, flat_array = np.unique(flat_seqs, return_inverse=True)\n",
    "assert order[0] == '-'\n",
    "print(flat_array.shape)\n",
    "replaced_seqs_T = flat_array.reshape(initial_shape)\n",
    "initial_weights = np.full(replaced_seqs_T[0].shape, fill_value=1./replaced_seqs_T[0].shape[0])\n",
    "print(initial_weights.shape)\n",
    "print(replaced_seqs_T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def total_ent_fxn(weights, seqs):\n",
    "    bin_counts = np.apply_along_axis(lambda x: np.bincount(x, weights=weights, minlength=22),\\\n",
    "                                     axis=1, arr=seqs)\n",
    "    all_ents = stats.entropy(bin_counts.T)\n",
    "    return -1*np.sum(all_ents)\n",
    "\n",
    "def total_ent_fxn_weighted(weights, seqs):\n",
    "    bin_counts = np.apply_along_axis(lambda x: np.bincount(x, weights=weights, minlength=22),\\\n",
    "                                     axis=1, arr=seqs)\n",
    "    trunc_bin_counts = bin_counts[:,1:]\n",
    "    ungapped_frac = np.sum(trunc_bin_counts, axis=1)/np.sum(bin_counts, axis=1)\n",
    "    all_ents = stats.entropy(trunc_bin_counts.T)\n",
    "    scaled_ents = all_ents*ungapped_frac\n",
    "    return -1*np.sum(scaled_ents)\n",
    "\n",
    "def total_ent_fxn_sim_ann(weights, seqs):\n",
    "    scaled_weights = weights/np.sum(weights)\n",
    "    bin_counts = np.apply_along_axis(lambda x: np.bincount(x, weights=weights, minlength=22),\\\n",
    "                                     axis=1, arr=seqs)\n",
    "    all_ents = stats.entropy(bin_counts.T)\n",
    "    return -1*np.sum(all_ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Says the sum of all variables must be zero\n",
    "cons = ({'type': 'eq', 'fun': lambda x: 1- np.sum(x)})\n",
    "options = {'maxiter':500}\n",
    "\n",
    "\n",
    "#Required to have non negative values\n",
    "bnds = tuple((10e-16,1) for x in initial_weights)\n",
    "\n",
    "# res = minimize(total_ent_fxn, initial_weights, args=(replaced_seqs_T),\\\n",
    "#                method='SLSQP', bounds=bnds, constraints=cons, options=options)\n",
    "res_gap = minimize(total_ent_fxn_weighted, initial_weights, args=(replaced_seqs_T),\\\n",
    "               method='SLSQP', bounds=bnds, constraints=cons, options=options)\n",
    "# res_sim_ann = basinhopping(total_ent_fxn_sim_ann, initial_weights,\\\n",
    "#                            minimizer_kwargs={\"method\": \"BFGS\", \"args\": replaced_seqs_T,\\\n",
    "#                                             \"options\": options})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res_gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(res.x, res_gap.x, 'bo')\n",
    "ax.plot([0,0.15], [0,0.15])\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(res.x, 30, alpha=0.5)\n",
    "ax.hist(res_gap.x, 30, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bin_counts = np.apply_along_axis(lambda x: np.bincount(x, weights=initial_weights, minlength=22),\\\n",
    "                                 axis=1, arr=replaced_seqs_T)\n",
    "gap_frac = bin_counts[:,0]/np.sum(bin_counts, axis=1)\n",
    "trunc_bin_counts = bin_counts[:,1:]\n",
    "i = stats.entropy(trunc_bin_counts.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trunc_bin_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sum(trunc_bin_counts.T, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trunc_bin_counts.T[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stats.entropy(trunc_bin_counts.T[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stats.entropy(np.bincount(replaced_seqs_T[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.bincount(replaced_seqs_T[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.bincount(replaced_seqs_T[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testy = np.apply_along_axis(lambda x: np.bincount(x, minlength=22), axis=1, arr=replaced_seqs_T[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "replaced_seqs_T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "blah = np.apply_along_axis(lambda x: stats.entropy(x), axis=1, arr = testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sum(blah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_ents = []\n",
    "for i in testy:\n",
    "    all_ents.append(stats.entropy(i))\n",
    "# print(np.sum(all_ents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hmm = stats.entropy(testy.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(hmm) == all_ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.isclose(hmm, all_ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bin_counts = np.apply_along_axis(lambda x: np.bincount(replaced_seqs_T, weights=weights, minlength=22),\\\n",
    "                                     axis=1, arr=seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "120px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
